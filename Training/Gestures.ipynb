{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from googletrans import Translator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1806 images belonging to 8 classes.\n",
      "Found 1806 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = r'Train'\n",
    "test_path = r'Test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x2000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(train_batches)\n",
    "\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "# Flatten the output for the fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add dense layers with activation functions\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 31, 31, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 15, 15, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                294976    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 414088 (1.58 MB)\n",
      "Trainable params: 414088 (1.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the Adam optimizer\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "181/181 [==============================] - 8s 38ms/step - loss: 0.5307 - accuracy: 0.9518 - val_loss: 0.2600 - val_accuracy: 0.8616 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "181/181 [==============================] - 7s 37ms/step - loss: 1.1081e-05 - accuracy: 1.0000 - val_loss: 0.2860 - val_accuracy: 0.8715 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "181/181 [==============================] - 7s 39ms/step - loss: 8.0351e-07 - accuracy: 1.0000 - val_loss: 0.2374 - val_accuracy: 0.8826 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "181/181 [==============================] - 7s 39ms/step - loss: 5.3202e-07 - accuracy: 1.0000 - val_loss: 0.2308 - val_accuracy: 0.8843 - lr: 2.0000e-04\n",
      "Epoch 5/10\n",
      "181/181 [==============================] - 7s 40ms/step - loss: 3.8172e-07 - accuracy: 1.0000 - val_loss: 0.2399 - val_accuracy: 0.8815 - lr: 2.0000e-04\n",
      "Epoch 6/10\n",
      "181/181 [==============================] - 7s 40ms/step - loss: 3.0093e-07 - accuracy: 1.0000 - val_loss: 0.2378 - val_accuracy: 0.8848 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to your data\n",
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop], validation_data=test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.100311778485775; accuracy of 89.99999761581421%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sneha Jain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# For getting next batch of testing imgs...\n",
    "imgs, labels = next(test_batches) \n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#Once the model is fitted we save the model using model.save()  function.\n",
    "\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {0: 'Bye', 1: 'Call Me', 2: 'Dislike', 3: 'Good Job', 4: 'Good Luck', 5: 'Peace', 6: 'Praying', 7: 'Rock On'}\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "desired_language = input(\"Enter the desired language: \")\n",
    "\n",
    "translated_dict = {}\n",
    "\n",
    "for key, value in word_dict.items():\n",
    "    translation = translator.translate(value, dest=desired_language)\n",
    "    translated_dict[key] = translation.text\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(translated_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(translated_dict[np.argmax(i)], end='   ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(r\"model.h5\")\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    _, thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Fetching contours in the frame (These contours can be of hand or any other object in foreground)...\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If the length of contours list = 0, it means we didn't get any contours...\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # The largest external contour should be the hand \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Returning the hand segment (max contour) and the thresholded image of the hand...\n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_gestures():\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    num_frames =0\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "    \n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        frame_copy = frame.copy()\n",
    "\n",
    "        # ROI from the frame\n",
    "        roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "        gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "\n",
    "        if num_frames < 70:\n",
    "        \n",
    "            cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",\n",
    "        (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "    \n",
    "        else: \n",
    "        # segmenting the hand region\n",
    "            hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "            if hand is not None:\n",
    "            \n",
    "                thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "                cv2.drawContours(frame_copy, [hand_segment + (ROI_right,ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "                cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "            \n",
    "                thresholded = cv2.resize(thresholded, (64, 64))\n",
    "                thresholded = cv2.cvtColor(thresholded, cv2.COLOR_GRAY2RGB)\n",
    "                thresholded = np.reshape(thresholded, (1, thresholded.shape[0], thresholded.shape[1], 3))\n",
    "            \n",
    "                pred = model.predict(thresholded)\n",
    "                cv2.putText(frame_copy, word_dict[np.argmax(pred)],(170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "    # Draw ROI on frame_copy\n",
    "            cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,ROI_bottom), (255,128,0), 3)\n",
    "\n",
    "    # incrementing the number of frames for tracking\n",
    "        num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "        cv2.putText(frame_copy, \"Sign Recognition\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "        cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "\n",
    "    # Close windows with Esc\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "# Release the camera and destroy all the windows\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
